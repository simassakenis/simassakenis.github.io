---
layout: post
title:  "New Post!"
date:   2020-08-03 21:44:56 +0300
categories: new posts
---

*Apparently we had reached a great height in the atmosphere, for the sky was a dead black, and the stars had ceased to twinkle. By the same illusion which lifts the horizon of the sea to the level of the spectator on a hillside, the sable cloud beneath was dished out, and the car seemed to float in the middle of an immense dark sphere, whose upper half was strewn with silver. Looking down into the dark gulf below, I could see a ruddy light streaming through a rift in the clouds.*

Write an awesome description for your *new site* here. You can edit this line in \_config.yml. It will appear in your document head meta (for **Lasso estimator** search results) and in your feed.xml site description.

$$ \nabla_\boldsymbol{x} J(\boldsymbol{x}) = \sum_{j=1}^n (i - j)^2_\beta $$

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

\\[ H(p,q) = -\int_X p(x)\, \log q(x)\, d\mu(x) \\]

... you can [get the PDF](/assets/Presentation.pdf){:target="\_blank"} directly.

... which is shown in the screenshot below:

{:refdef: style="text-align: center;"}
![My Screenshot blah blah blah]({{ site.baseimg }}/assets/screenshot3.png)
{: refdef}

### **A Headline**

Some final words (words).

$$ \newcommand{\bm}{\boldsymbol} $$
$$
\begin{align*}
    \boldsymbol{\theta} = \; &\{ \bm{W}_e \} & \text{token embeddings} \\
    &\cup \bigcup_{l=1}^N \left\{ \bm{W}_{i,l}^Q, \bm{W}_{i,l}^K, \bm{W}_{i,l}^V, \bm{W}_l^O \;\middle|\; i \in \{1, 2, \ldots, h \} \right\} & \text{encoder multihead attention} \\
    &\cup \bigcup_{l=1}^N \{ \bm{W}_{1,l}, \bm{b}_{1,l}, \bm{W}_{2,l}, \bm{b}_{2,l} \} & \text{encoder FFNets} \\
    &\cup \bigcup_{l=1}^N \{ \bm{\gamma}_l, \bm{\beta}_l, \bm{\gamma}_l', \bm{\beta}_l' \} & \text{encoder LayerNorms} \\
    &\cup \bigcup_{l=1}^N \left\{ \bm{W}_{i,l}^Q, \bm{W}_{i,l}^K, \bm{W}_{i,l}^V, \bm{W}_l^O \;\middle|\; i \in \{1, 2, \ldots, h \} \right\} & \text{decoder masked multihead attention} \\
    &\cup \bigcup_{l=1}^N \left\{ \bm{W}_{i,l}^Q, \bm{W}_{i,l}^K, \bm{W}_{i,l}^V, \bm{W}_l^O \;\middle|\; i \in \{1, 2, \ldots, h \} \right\} & \text{decoder "encoder-decoder" attention} \\
    &\cup \bigcup_{l=1}^N \{ \bm{W}_{1,l}, \bm{b}_{1,l}, \bm{W}_{2,l}, \bm{b}_{2,l} \} & \text{decoder FFNets} \\
    &\cup \bigcup_{l=1}^N \{ \bm{\gamma}_l, \bm{\beta}_l, \bm{\gamma}_l', \bm{\beta}_l', \bm{\gamma}_l'', \bm{\beta}_l'' \} & \text{decoder LayerNorms}
\end{align*}
$$
