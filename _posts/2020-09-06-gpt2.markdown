---
layout: post
title:  "The Formulated GPT-2"
date:   2020-10-22 15:44:56 +0300
---

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

Short, self-contained, complete.
$$
\newcommand{\bm}{\boldsymbol}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$


&nbsp;
## Modules

<div class="boxed">
$$ \textbf{Diagonal Matrix} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{diag} : \mathbb{R}^n \to \mathbb{R}^{n \times n}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{diag}(\bm{x})_{i,j} = \begin{cases} x_i & \text{if } i = j, \\ 0 & \text{otherwise}. \end{cases}
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Stack} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{Stack}_n : \mathbb{R}^H \to \mathbb{R}^{n \times H}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{Stack}_n(\bm{x})_{i,j} = x_j.
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{One-Hot Encoding} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{OneHot} : \{ 1, 2, \ldots, V \}^n \to \{ 0, 1 \}^{n \times V}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{OneHot}(\bm{x})_{i,j} = \begin{cases} 1 & \text{if } j = x_i, \\ 0 & \text{otherwise}. \end{cases}
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Softmax} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{Softmax} : \mathbb{R}^{n \times H} \to [0, 1]^{n \times H}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{Softmax}(\bm{X})_{i,j} = \frac{e^{X_{i,j}}}{\sum_{k=1}^{H} e^{X_{i,k}}}.
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Bidirectional Attention Mask} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{Mask}^\text{BD} \in \mathbb{R}^{n \times n}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle (\text{Mask}^\text{BD})_{i,j} = 1
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Autoregressive Attention Mask} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{Mask}^\text{AR} \in \mathbb{R}^{n \times n}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle (\text{Mask}^\text{AR})_{i,j} = \begin{cases} -\infty & \text{if } j > i, \\ 1 & \text{otherwise}. \end{cases} \\
    \text{Example:} &\hspace{10pt} \displaystyle \text{For } n = 4, \text{ } \text{Mask}^\text{AR} = \begin{bmatrix} 1 & -\infty & -\infty & -\infty \\ 1 & 1 & -\infty & -\infty \\ 1 & 1 & 1 & -\infty \\ 1 & 1 & 1 & 1 \end{bmatrix}.
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Masked Scaled Dot-Product Attention} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{Attention} : \mathbb{R}^{n \times D} \times \mathbb{R}^{n \times D} \times \mathbb{R}^{n \times D} \times \mathbb{R}^{n \times n} \to \mathbb{R}^{n \times D}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{Attention}(\bm{Q}, \bm{K}, \bm{V}, \text{Mask}) = \text{Softmax}\left( \frac{\text{Mask} \odot \bm{Q} \bm{K}^\top}{\sqrt{D}} \right) \bm{V}. \\
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Concatenation} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{Concat} : \mathbb{R}^{n \times D} \times \cdots \times \mathbb{R}^{n \times D} \to \mathbb{R}^{n \times AD}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{Concat}(\bm{X}_1, \ldots, \bm{X}_h)_{:, (k-1) D + 1: k D} = \bm{X}_k \quad (k = 1, 2, \ldots, A). \\
    \text{Illustration:} &\hspace{10pt} \displaystyle \text{Concat}\left( \begin{bmatrix} \cdot & \cdot \\ \cdot & \cdot \end{bmatrix}, \begin{bmatrix} \cdot & \cdot \\ \cdot & \cdot \end{bmatrix}, \begin{bmatrix} \cdot & \cdot \\ \cdot & \cdot \end{bmatrix} \right) = \begin{bmatrix} \cdot & \cdot & \cdot & \cdot & \cdot & \cdot \\ \cdot & \cdot & \cdot & \cdot & \cdot & \cdot \end{bmatrix}.
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Masked Multi-Head Self-Attention} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{MultiHeadSA} : \mathbb{R}^{n \times H} \times \mathbb{R}^{n \times n} \to \mathbb{R}^{n \times H}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{MultiHeadSA}(\bm{X}, \text{Mask}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) \bm{W}^{O}, \quad \text{where} \\
    &\hspace{10pt} \displaystyle \text{head}_i = \text{Attention}(\bm{X} \bm{W}_i^{Q}, \bm{X} \bm{W}_i^{K}, \bm{X} \bm{W}_i^{V}, \text{Mask}). \\
    \text{Parameters:} &\hspace{10pt} \displaystyle \bm{W}_i^{Q} \in \mathbb{R}^{H \times D} \quad \text{for } i \in \{ 1, 2, \ldots, A\}, \\
    &\hspace{10pt} \displaystyle \bm{W}_i^{K} \in \mathbb{R}^{H \times D} \quad \text{for } i \in \{ 1, 2, \ldots, A\}, \\
    &\hspace{10pt} \displaystyle \bm{W}_i^{V} \in \mathbb{R}^{H \times D} \quad \text{for } i \in \{ 1, 2, \ldots, A\}, \\
    &\hspace{10pt} \displaystyle \bm{W}^{O} \in \mathbb{R}^{AD \times H}.
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Feed-Forward ReLU Network} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{FFN}_\text{ReLU} : \mathbb{R}^{n \times H} \to \mathbb{R}^{n \times H}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{FFN}_\text{ReLU}(\bm{X}) = \text{max}(0, \bm{X} \bm{W}_1 + \bm{b}_1) \bm{W}_2 + \bm{b}_2. \\
    \text{Parameters:} &\hspace{10pt} \displaystyle \bm{W}_1 \in \mathbb{R}^{H \times F}, \; \bm{b}_1 \in \mathbb{R}^{F}, \; \bm{W}_2 \in \mathbb{R}^{F \times H}, \; \bm{b}_2 \in \mathbb{R}^{H}.
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Feed-Forward GELU Network} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{FFN}_\text{GELU} : \mathbb{R}^{n \times H} \to \mathbb{R}^{n \times H}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{FFN}_\text{GELU}(\bm{X}) = \text{GELU}(\bm{X} \bm{W}_1 + \bm{b}_1) \bm{W}_2 + \bm{b}_2, \quad \text{where} \\
    &\hspace{10pt} \displaystyle \text{GELU}(\bm{X}) = \bm{X} \odot  \text{sigmoid}(1.702 \bm{X}), \quad \text{and} \\
    &\hspace{10pt} \displaystyle \text{sigmoid}(x) = \frac{1}{1 + e^{-x}}. \\
    \text{Parameters:} &\hspace{10pt} \displaystyle \bm{W}_1 \in \mathbb{R}^{H \times F}, \; \bm{b}_1 \in \mathbb{R}^{F}, \; \bm{W}_2 \in \mathbb{R}^{F \times H}, \; \bm{b}_2 \in \mathbb{R}^{H}.
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Layer normalization} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{LayerNorm} : \mathbb{R}^{n \times H} \to \mathbb{R}^{n \times H}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{LayerNorm}(\bm{X})_{i,j} = \gamma_j \left( \frac{X_{i,j} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}} \right) + \beta_j, \quad \text{where} \\
    &\hspace{10pt} \displaystyle \mu_i = \frac{1}{H} \sum_{j=1}^{H} X_{i,j}, \quad \sigma_i^2 = \frac{1}{H} \sum_{j=1}^{H} (X_{i,j} - \mu_i)^2. \\
    \text{Parameters:} &\hspace{10pt} \displaystyle \bm{\gamma} \in \mathbb{R}^{H}, \; \bm{\beta} \in \mathbb{R}^{H}.
\end{array}
$$
</div>


&nbsp;
## Definition

The Generative Pre-trained Transformer (GPT) is a model for next-symbol prediction. That is, it takes a sequence of symbols as input and outputs a probability distribution for the next symbol given that sequence. Each symbol is represented by a number from $$ \{ 1, 2, \ldots, V \} $$, corresponding to its ID in a pre-specified vocabulary consisting of $$ s $$ items. The input to the GPT is thus a sequence of $$ n $$ numbers $$ \bm{x} = (x_1, \ldots, x_n) \in \{1, 2, \ldots, V\}^n $$, and the output is a sequence of $$ V $$ numbers $$ \text{GPT}(\bm{x}) \in [0, 1]^{V} $$ giving the probabilities for each vocabulary item being the next symbol in the sequence. GPT can thus be defined as a function $$ \text{GPT} : \{ 1, 2, \ldots, V \}^n \to [0, 1]^{V} $$ that maps inputs to outputs by performing the following computations:

<div class="boxed">
$$ \textbf{GPT} $$
$$
\begin{array}{@{}lll}
     \text{Input} & \bm{x} = (x_1, \ldots, x_n) \in \{1, 2, \ldots, V\}^{n} &\hspace{10pt} (n) \\
     \text{Embedding} & \bm{X}^{(0)} = \text{OneHot}(\bm{x}) \bm{W}_e + \bm{W}_p &\hspace{10pt} (n \times H) \\
     \text{Layers} & \bm{X}'^{(l)} = \text{LayerNorm}(\text{MultiHeadSA}(\bm{X}^{(l-1)}, \text{Mask}^\text{AR}) + \bm{X}^{(l-1)}), &\hspace{10pt} (n \times H) \\
     & \bm{X}^{(l)} = \text{LayerNorm}(\text{FFN}_\text{GELU}(\bm{X}'^{(l)}) + \bm{X}'^{(l)}) \quad \text{for } l = 1, \ldots, L&\hspace{10pt} (n \times H) \\
     \text{Output} & \text{GPT}(\bm{x}) = \text{Softmax}\left( \bm{X}_{n,:}^{(L)} \bm{W}_e^\top \right) &\hspace{10pt} (V)
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{GPT-2} $$
$$
\begin{array}{@{}lll}
     \text{Input} & \bm{x} = (x_1, \ldots, x_n) \in \{1, 2, \ldots, V\}^{n} &\hspace{10pt} (n) \\
     \text{Embedding} & \bm{X}^{(0)} = \text{OneHot}(\bm{x}) \bm{W}_e + \bm{W}_p &\hspace{10pt} (n \times H) \\
     \text{Layers} & \bm{X}'^{(l)} = \text{MultiHeadSA}(\text{LayerNorm}(\bm{X}^{(l-1)}), \text{Mask}^\text{AR}) + \bm{X}^{(l-1)}, &\hspace{10pt} (n \times H) \\
     & \bm{X}^{(l)} = \text{FFN}_\text{GELU}(\text{LayerNorm}(\bm{X}'^{(l)})) + \bm{X}'^{(l)} \quad \text{for } l = 1, \ldots, L &\hspace{10pt} (n \times H) \\
     & \bm{X}^{(L)} = \text{LayerNorm}(\bm{X}^{(L)}) &\hspace{10pt} (n \times H) \\
     \text{Output} & \text{GPT}(\bm{x}) = \text{Softmax}\left( \bm{X}_{n,:}^{(L)} \bm{W}_e^\top \right) &\hspace{10pt} (V)
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{GPT-3} $$
$$
\begin{array}{@{}lll}
     \text{Input} & \bm{x} = (x_1, \ldots, x_n) \in \{1, 2, \ldots, V\}^{n} &\hspace{10pt} (n) \\
     \text{Embedding} & \bm{X}^{(0)} = \text{OneHot}(\bm{x}) \bm{W}_e + \bm{W}_p &\hspace{10pt} (n \times H) \\
     \text{Layers} & \text{Mask}^{(l)} = \begin{cases} \text{Mask}^\text{AR} & \text{if } l \text{ is even}, \\ \text{Mask}^\text{ST} & \text{if } l \text{ is odd}; \end{cases} &\hspace{10pt} (n \times H) \\
     & \bm{X}'^{(l)} = \text{MultiHeadSA}(\text{LayerNorm}(\bm{X}^{(l-1)}), \text{Mask}^{(l)}) + \bm{X}^{(l-1)}, &\hspace{10pt} (n \times H) \\
     & \bm{X}^{(l)} = \text{FFN}_\text{GELU}(\text{LayerNorm}(\bm{X}'^{(l)})) + \bm{X}'^{(l)} \quad \text{for } l = 1, \ldots, L &\hspace{10pt} (n \times H) \\
     & \bm{X}^{(L)} = \text{LayerNorm}(\bm{X}^{(L)}) &\hspace{10pt} (n \times H) \\
     \text{Output} & \text{GPT}(\bm{x}) = \text{Softmax}\left( \bm{X}_{n,:}^{(L)} \bm{W}_e^\top \right) &\hspace{10pt} (V)
\end{array}
$$
</div>

Here, $$ \bm{W}_e \in \mathbb{R}^{V \times H} $$ and $$ \bm{W}_p \in \mathbb{R}^{n \times H} $$ are the learned embedding and positional encoding matrices, $$ \bm{X}_0 $$ is the embedding of the input sequence, and $$ \bm{X}_1, \ldots, \bm{X}_{L} $$ are the hidden layer representations of the input sequence. Also, due to autoregressive masking, we have that

$$ \text{Softmax}\left( \bm{X}^{(L)} \bm{W}_e^\top \right) =
\begin{bmatrix}
    \rule[.5ex]{8ex}{0.5pt} & P(X_2 \mid X_1 = x_1; \bm{\theta}) & \rule[.5ex]{8ex}{0.5pt} \\
    \rule[.5ex]{8ex}{0.5pt} & P(X_3 \mid X_1 = x_1, X_2 = x_2; \bm{\theta}) & \rule[.5ex]{8ex}{0.5pt} \\
    & \vdots & \\
    \rule[.5ex]{8ex}{1pt} & P(X_{n+1} \mid X_1 = x_1, \ldots, X_n = x_n; \bm{\theta}) & \rule[.5ex]{8ex}{1pt}
\end{bmatrix}. $$

The training objective is to maximize the likelihood of observed data $$ (x_2, \ldots, x_n) $$ conditional on $$ X_1 = x_1 $$, which corresponds to minimizing the negative log-likelihood loss given by

$$
\begin{align*}
    \text{loss} &= - \log{P(X_2 = x_2, \ldots, X_n = x_n \mid X_1 = x_1; \bm{\theta})} \\
    &= - \sum_{j=2}^n \log{P(X_j = x_j \mid X_1 = x_1, \ldots, X_{j-1} = x_{j-1}; \bm{\theta})} \\
    &= - \sum_{j=2}^n \log\left( \text{Softmax}\left( \bm{X}_{j-1,:}^{(L)} \bm{W}_e^\top \right) \right).
\end{align*}
$$


&nbsp;
## BERT


Given a pair of sentences $$ \bm{x}^A = (x_1^A, \ldots, x_{n_A}^A) $$ and $$ \bm{x}^B = (x_1^B, \ldots, x_{n_B}^B) $$, and an index $$ j^\text{masked} $$ at which to mask a token, let

$$ \bm{x} = (i^\text{cls}, x_1^A, \ldots, x_{n_A}^A, i^\text{sep}, x_1^B, \ldots, x_{n_B}^B, i^\text{sep} ) $$

and $$ \bm{x}' = (x'_1, \ldots, x'_n) $$ such that

$$ x'_i = \begin{cases} i^\text{mask} & \text{if } i = j^\text{masked}, \\ x_i & \text{otherwise}, \end{cases} $$

where $$ i^\text{cls} $$, $$ i^\text{sep} $$, and $$ i^\text{mask} $$ are the vocabulary indices of the classifier ($$ \texttt{[CLS]} $$), separator ($$ \texttt{[SEP]} $$), and mask ($$ \texttt{[MASK]} $$) tokens, and $$ n = n_A + n_B + 3 $$. The sequence $$ \bm{t} = (t_1, \ldots, t_n) $$ of token IDs (indicating which of the two input sentences each token belongs to) is then given by

$$ t_i = \begin{cases} 0 & \text{if } i \leq n_A + 2, \\ 1 & \text{otherwise}. \end{cases} $$

<div class="boxed">
$$ \textbf{BERT} $$
$$
\begin{array}{@{}lll}
     \text{Input} & \bm{x}' = (x'_1, \ldots, x'_n) \in \{1, 2, \ldots, V\}^{n} &\hspace{10pt} (n) \\
     & \bm{t} = (t_1, \ldots, t_n) \in \{0, 1\}^{n} &\hspace{10pt} (n) \\
     & j^\text{masked} \in \{1, 2, \ldots, n_A + n_B\} &\hspace{10pt} (1) \\
     \text{Embedding} & \bm{X}'^{(0)} = \text{OneHot}(\bm{x}') \bm{W}_e + \bm{W}_p &\hspace{10pt} (n \times H) \\
     &\bm{X}^{(0)} = \bm{X}'^{(0)} + \text{diag}(\bm{t}) \text{Stack}_n(\bm{w}^A) + \text{diag}(1 - \bm{t}) \text{Stack}_n(\bm{w}^B) &\hspace{10pt} (n \times H) \\
     \text{Layers} & \bm{X}'^{(l)} = \text{LayerNorm}(\text{MultiHeadSA}(\bm{X}^{(l-1)}, \text{Mask}^\text{BD}) + \bm{X}^{(l-1)}), &\hspace{10pt} (n \times H) \\
     & \bm{X}^{(l)} = \text{LayerNorm}(\text{FFN}_\text{GELU}(\bm{X}'^{(l)}) + \bm{X}'^{(l)}) \quad \text{for } l = 1, \ldots, L&\hspace{10pt} (n \times H) \\
     \text{Output} & \text{BERT}(\bm{x}) = \text{Softmax}\left( \bm{X}_{j^\text{masked},:}^{(L)} \bm{W}_e^\top \right) &\hspace{10pt} (V)
\end{array}
$$
</div>


RoBERTa does not require token type IDs (due to the removal of the next sentence prediction objective) and uses $$ \texttt{[<s>]} $$, $$ \texttt{[</s>]} $$, and $$ \texttt{[<mask>]} $$ as classifier, separator, and mask tokens.


<div class="boxed">
$$ \textbf{RoBERTa} $$
$$
\begin{array}{@{}lll}
     \text{Input} & \bm{x}' = (x'_1, \ldots, x'_n) \in \{1, 2, \ldots, V\}^{n} &\hspace{10pt} (n) \\
     & j^\text{masked}  \in \{1, 2, \ldots, n\} &\hspace{10pt} (1) \\
     \text{Embedding} & \bm{X}^{(0)} = \text{OneHot}(\bm{x}') \bm{W}_e + \bm{W}_p &\hspace{10pt} (n \times H) \\
     \text{Layers} & \bm{X}'^{(l)} = \text{LayerNorm}(\text{MultiHeadSA}(\bm{X}^{(l-1)}, \text{Mask}^\text{BD}) + \bm{X}^{(l-1)}), &\hspace{10pt} (n \times H) \\
     & \bm{X}^{(l)} = \text{LayerNorm}(\text{FFN}_\text{GELU}(\bm{X}'^{(l)}) + \bm{X}'^{(l)}) \quad \text{for } l = 1, \ldots, L&\hspace{10pt} (n \times H) \\
     \text{Output} & \text{RoBERTa}(\bm{x}) = \text{Softmax}\left( \bm{X}_{j^\text{masked},:}^{(L)} \bm{W}_e^\top \right) &\hspace{10pt} (V)
\end{array}
$$
</div>

The training objective is to maximize the likelihood of the masked tokens conditional on the full (bidirectional) context, which corresponds to minimizing the negative log-likelihood loss given by

$$
\begin{align*}
    \text{loss} &= - \log{P(X_{j_1} = x_{j_1}, \ldots, X_{j_m} = x_{j_m} \mid X'_1 = x'_1, \ldots, X'_n = x'_n; \bm{\theta})} \\
    &= - \sum_{i = 1}^m \log{P(X_{j_i} = x_{j_i} \mid X'_1 = x'_1, \ldots, X'_n = x'_n; \bm{\theta})} \\
    &= - \sum_{i = 1}^m \log\left( \text{Softmax}\left( \bm{X}_{j_i,:}^{(L)} \bm{W}_e^\top \right) \right),
\end{align*}
$$

under the assumption that $$ X_{j_p} $$ is independent of $$ X_{j_q} $$ given $$ (X'_1 = x'_1, \ldots, X'_n = x'_n) $$. Here $$ j_1, \ldots, j_m $$ are the indices of the masked tokens.


&nbsp;
## Statistical interpretation


Suppose we have a vocabulary consisting of $$ V $$ distinct tokens (e.g., words of a language, letters of an alphabet, intensities of pixels). Let $$ X_1, X_2, X_3, \ldots $$ be a seqeunce of random variables representing a sequence of tokens, where $$ X_j \in \{ 1, 2, \ldots, V \} $$ is the vocabulary ID of the $$ j $$-th token. Generative Pre-trained Transformer (GPT) is a model for the conditional distribution of $$ X_{n+1} \mid (X_1, \ldots, X_n) $$, i.e., the categorical distribution for the next token given a sequence. Bidirectional Encoder Representations from Transformers (BERT) is a model for the conditional distribution of $$ X_{j} \mid (X'_1, \ldots, X'_n) $$, i.e., the categorical distribution for the masked token given a corrupted sequence ($$ X_j' = I i^\text{mask} + (1 - I) X_j $$ where $$ I \sim \text{Bern}(p) $$ for some $$ p \in [0, 1] $$). XLNet is a model for the conditional distribution of $$ X_{z_{t+1}} \mid (X_{z_1}, \ldots, X_{z_t}) $$, i.e., the categorical distribution for the next token given the preceding tokens according to a permutation $$ (z_1, \ldots, z_{t+1}) \subseteq \{ 1, \ldots, n \}^{t+1} $$ of token positions (with $$ t < n $$).

Let $$ M $$ be a set of indices indicating which tokens have been masked in the corrupted sequence $$ \bm{X}' $$, and let $$ (z_1, \ldots, z_n) $$ be a some permutation of $$ \{ 1, \ldots, n \} $$. The training objectives for the different models can then be summarized as follows:

* BERT: maximize

$$
\begin{align*}
    \log{P(\bm{X}_M = \bm{x}_M; \bm{\theta})} = \sum_{j \in M} \log{P(X_j = x_j \mid X'_1 = x'_1, \ldots, X'_n = x'_n; \bm{\theta})}.
\end{align*}
$$

* GPT: maximize

$$
\begin{align*}
    \log{P(\bm{X} = \bm{x}; \bm{\theta})} = \sum_{j=1}^n \log{P(X_j = x_j \mid X_1 = x_1, \ldots, X_{j-1} = x_{j-1}; \bm{\theta})}.
\end{align*}
$$

* XLNet: maximize

$$
\begin{align*}
    \log{P(\bm{X} = \bm{x}; \bm{\theta})} = \sum_{j=1}^n \log{P(X_{z_j} = x_{z_j} \mid X_{z_1} = x_{z_1}, \ldots, X_{z_{j-1}} = x_{z_{j-1}}; \bm{\theta})}.
\end{align*}
$$



&nbsp;
## Parameters


The set of all parameters in the GPT is given by:

$$
\begin{align*}
    \theta = \; &\{ \bm{W}_e, \bm{W}_p \} & \text{embeddings} \\
    &\cup \bigcup_{l=1}^{L} \left\{ \bm{W}_{i,l}^Q, \bm{W}_{i,l}^K, \bm{W}_{i,l}^V, \bm{W}_l^O \;\middle|\; i \in \{1, 2, \ldots, A\} \right\} & \text{MultiHeadSAs} \\
    &\cup \bigcup_{l=1}^{L} \{ \bm{W}_{1,l}, \bm{b}_{1,l}, \bm{W}_{2,l}, \bm{b}_{2,l} \} & \text{FFNets} \\
    &\cup \bigcup_{l=1}^{L} \{ \bm{\gamma}_l, \bm{\beta}_l, \bm{\gamma}_l', \bm{\beta}_l' \} & \text{LayerNorms}.
\end{align*}
$$

Correspondingly, the total number of parameters in the Transformer is

$$
\begin{align*}
    \lambda = &\; V \cdot H + n \cdot H & \text{embeddings} \\
    &+ L \cdot ( A\cdot (H \cdot D + H \cdot D + H \cdot D) + AD \cdot H ) & \text{MultiHeadSAs} \\
    &+ L \cdot ( H \cdot F + F + F \cdot H + H ) & \text{FFNets} \\
    &+ L \cdot ( H + H + H + H ). & \text{LayerNorms}
\end{align*}
$$


&nbsp;
## Hyperparameters


Vocabulary:

$$
\begin{align*}
    V &= 40\,478 & \text{byte-pair encoding.}
\end{align*}
$$

Epsilon in Layer Normalization:

$$
\begin{align*}
    \epsilon
\end{align*}
$$


Hyperparameter values (used in the paper):

$$
\begin{align*}
    & H = 768 & \text{dimension of the residual stream} \\
    & F = 3072 & \text{dimension of the hidden layer in feed-forward nets} \\
    & D = 64 & \text{dimension of the vector space for queries, keys, and values} \\
    & L = 12 & \text{number of layers} \\
    & A = 12 & \text{number of heads in multihead attention} \\
    & V = 40\,478 & \text{number of items in the vocabulary} \\
    & n_\text{ctx} = 512 & \text{context size}
\end{align*}
$$

With these values we get $$ \lambda = 116\,497\,920 $$.
