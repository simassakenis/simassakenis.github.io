---
layout: post
title:  "The Formulated GPT-2"
date:   2020-10-22 15:44:56 +0300
---

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

Short, self-contained, complete.
$$
\newcommand{\bm}{\boldsymbol}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$


&nbsp;
## Modules

<div class="boxed">
$$ \textbf{Diagonal Matrix} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{diag} : \mathbb{R}^n \to \mathbb{R}^{n \times n}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{diag}(\bm{x})_{i,j} = \begin{cases} x_i & \text{if } i = j, \\ 0 & \text{otherwise}. \end{cases}
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Stack} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{Stack}_n : \mathbb{R}^H \to \mathbb{R}^{n \times H}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{Stack}_n(\bm{x})_{i,j} = x_j.
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{One-Hot Encoding} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{OneHot} : \{ 1, 2, \ldots, V \}^n \to \{ 0, 1 \}^{n \times V}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{OneHot}(\bm{x})_{i,j} = \begin{cases} 1 & \text{if } j = x_i, \\ 0 & \text{otherwise}. \end{cases}
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Permutation of Rows} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{Perm} : \mathbb{R}^{n \times p} \times \mathcal{Z}_n \to \mathbb{R}^{n \times p}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{Perm}(\bm{X}, \bm{z})[i, j] = \bm{X}[z_i, j].
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Softmax} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{Softmax} : \mathbb{R}^{n \times H} \to [0, 1]^{n \times H}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{Softmax}(\bm{X})_{i,j} = \frac{e^{X_{i,j}}}{\sum_{k=1}^{H} e^{X_{i,k}}}.
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Bidirectional Attention Mask} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{Mask}^\text{BD} \in \mathbb{R}^{n \times n}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle (\text{Mask}^\text{BD})_{i,j} = 1
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Autoregressive Attention Mask (next)} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{Mask}_\text{next}^\text{AR} \in \mathbb{R}^{n \times n}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle (\text{Mask}_\text{next}^\text{AR})_{i,j} = \begin{cases} -\infty & \text{if } j > i, \\ 1 & \text{otherwise}. \end{cases} \\
    \text{Example:} &\hspace{10pt} \displaystyle \text{For } n = 4, \text{ } \text{Mask}_\text{next}^\text{AR} = \begin{bmatrix} 1 & -\infty & -\infty & -\infty \\ 1 & 1 & -\infty & -\infty \\ 1 & 1 & 1 & -\infty \\ 1 & 1 & 1 & 1 \end{bmatrix}.
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Autoregressive Attention Mask (current)} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{Mask}_\text{curr}^\text{AR} \in \mathbb{R}^{n \times n}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle (\text{Mask}_\text{curr}^\text{AR})_{i,j} = \begin{cases} -\infty & \text{if } j \geq i, \\ 1 & \text{otherwise}. \end{cases} \\
    \text{Example:} &\hspace{10pt} \displaystyle \text{For } n = 4, \text{ } \text{Mask}_\text{curr}^\text{AR} = \begin{bmatrix} -\infty & -\infty & -\infty & -\infty \\ 1 & -\infty & -\infty & -\infty \\ 1 & 1 & -\infty & -\infty \\ 1 & 1 & 1 & -\infty \end{bmatrix}.
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Masked Scaled Dot-Product Attention} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{Attention} : \mathbb{R}^{n \times D} \times \mathbb{R}^{n \times D} \times \mathbb{R}^{n \times D} \times \mathbb{R}^{n \times n} \to \mathbb{R}^{n \times D}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{Attention}(\bm{Q}, \bm{K}, \bm{V}, \text{Mask}) = \text{Softmax}\left( \frac{\text{Mask} \odot \bm{Q} \bm{K}^\top}{\sqrt{D}} \right) \bm{V}. \\
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Concatenation} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{Concat} : \mathbb{R}^{n \times D} \times \cdots \times \mathbb{R}^{n \times D} \to \mathbb{R}^{n \times AD}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{Concat}(\bm{X}_1, \ldots, \bm{X}_h)_{:, (k-1) D + 1: k D} = \bm{X}_k \quad (k = 1, 2, \ldots, A). \\
    \text{Illustration:} &\hspace{10pt} \displaystyle \text{Concat}\left( \begin{bmatrix} \cdot & \cdot \\ \cdot & \cdot \end{bmatrix}, \begin{bmatrix} \cdot & \cdot \\ \cdot & \cdot \end{bmatrix}, \begin{bmatrix} \cdot & \cdot \\ \cdot & \cdot \end{bmatrix} \right) = \begin{bmatrix} \cdot & \cdot & \cdot & \cdot & \cdot & \cdot \\ \cdot & \cdot & \cdot & \cdot & \cdot & \cdot \end{bmatrix}.
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Masked Multi-Head Self-Attention} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{MH} : \mathbb{R}^{n \times H} \times \mathbb{R}^{n \times n} \to \mathbb{R}^{n \times H}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{MH}(\bm{X}, \text{Mask}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_A) \bm{W}^{O}, \quad \text{where} \\
    &\hspace{10pt} \displaystyle \text{head}_i = \text{Attention}(\bm{X} \bm{W}_i^{Q}, \bm{X} \bm{W}_i^{K}, \bm{X} \bm{W}_i^{V}, \text{Mask}). \\
    \text{Parameters:} &\hspace{10pt} \displaystyle \bm{W}_i^{Q} \in \mathbb{R}^{H \times D} \quad \text{for } i \in \{ 1, 2, \ldots, A\}, \\
    &\hspace{10pt} \displaystyle \bm{W}_i^{K} \in \mathbb{R}^{H \times D} \quad \text{for } i \in \{ 1, 2, \ldots, A\}, \\
    &\hspace{10pt} \displaystyle \bm{W}_i^{V} \in \mathbb{R}^{H \times D} \quad \text{for } i \in \{ 1, 2, \ldots, A\}, \\
    &\hspace{10pt} \displaystyle \bm{W}^{O} \in \mathbb{R}^{AD \times H}.
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Two-Stream Self-Attention} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{TwoStreamSA} : \mathbb{R}^{n \times H} \times \mathbb{R}^{n \times H} \times \mathcal{Z}_n \to \mathbb{R}^{n \times H} \times \mathbb{R}^{n \times H}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{TwoStreamSA}(\bm{H}, \bm{G}, \bm{z}) = \begin{pmatrix} \text{Concat}(\text{head}_1^h, \ldots, \text{head}_A^h) \bm{W}^{O}, \\ \text{Concat}(\text{head}_1^g, \ldots, \text{head}_A^g) \bm{W}^{O} \end{pmatrix}, \text{ where} \\
    &\hspace{10pt} \displaystyle \text{head}_i^h = \text{Attention}(\bm{H} \bm{W}_i^{Q}, \bm{H} \bm{W}_i^{K}, \bm{H} \bm{W}_i^{V}, \text{Perm}(\text{Mask}_\text{next}^\text{AR}, \bm{z})), \\
    &\hspace{10pt} \displaystyle \text{head}_i^g = \text{Attention}(\bm{G} \bm{W}_i^{Q}, \bm{H} \bm{W}_i^{K}, \bm{H} \bm{W}_i^{V}, \text{Perm}(\text{Mask}_\text{curr}^\text{AR}, \bm{z})). \\
    \text{Parameters:} &\hspace{10pt} \displaystyle \bm{W}_i^{Q} \in \mathbb{R}^{H \times D} \quad \text{for } i \in \{ 1, 2, \ldots, A\}, \\
    &\hspace{10pt} \displaystyle \bm{W}_i^{K} \in \mathbb{R}^{H \times D} \quad \text{for } i \in \{ 1, 2, \ldots, A\}, \\
    &\hspace{10pt} \displaystyle \bm{W}_i^{V} \in \mathbb{R}^{H \times D} \quad \text{for } i \in \{ 1, 2, \ldots, A\}, \\
    &\hspace{10pt} \displaystyle \bm{W}^{O} \in \mathbb{R}^{AD \times H}.
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Feed-Forward ReLU Network} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{FF}^\text{ReLU} : \mathbb{R}^{n \times H} \to \mathbb{R}^{n \times H}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{FF}^\text{ReLU}(\bm{X}) = \text{max}(0, \bm{X} \bm{W}_1 + \bm{b}_1) \bm{W}_2 + \bm{b}_2. \\
    \text{Parameters:} &\hspace{10pt} \displaystyle \bm{W}_1 \in \mathbb{R}^{H \times F}, \; \bm{b}_1 \in \mathbb{R}^{F}, \; \bm{W}_2 \in \mathbb{R}^{F \times H}, \; \bm{b}_2 \in \mathbb{R}^{H}.
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Feed-Forward GELU Network} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{FF}^\text{GELU} : \mathbb{R}^{n \times H} \to \mathbb{R}^{n \times H}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{FF}^\text{GELU}(\bm{X}) = \text{GELU}(\bm{X} \bm{W}_1 + \bm{b}_1) \bm{W}_2 + \bm{b}_2, \quad \text{where} \\
    &\hspace{10pt} \displaystyle \text{GELU}(\bm{X}) = \bm{X} \odot  \text{sigmoid}(1.702 \bm{X}), \quad \text{and} \\
    &\hspace{10pt} \displaystyle \text{sigmoid}(x) = \frac{1}{1 + e^{-x}}. \\
    \text{Parameters:} &\hspace{10pt} \displaystyle \bm{W}_1 \in \mathbb{R}^{H \times F}, \; \bm{b}_1 \in \mathbb{R}^{F}, \; \bm{W}_2 \in \mathbb{R}^{F \times H}, \; \bm{b}_2 \in \mathbb{R}^{H}.
\end{array}
$$
</div>

<div class="boxed">
$$ \textbf{Layer normalization} $$
$$
\begin{array}{ll}
    \text{Signature:} &\hspace{10pt} \displaystyle \text{LN} : \mathbb{R}^{n \times H} \to \mathbb{R}^{n \times H}. \\
    \text{Definition:} &\hspace{10pt} \displaystyle \text{LN}(\bm{X})_{i,j} = \gamma_j \left( \frac{X_{i,j} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}} \right) + \beta_j, \quad \text{where} \\
    &\hspace{10pt} \displaystyle \mu_i = \frac{1}{H} \sum_{j=1}^{H} X_{i,j}, \quad \sigma_i^2 = \frac{1}{H} \sum_{j=1}^{H} (X_{i,j} - \mu_i)^2. \\
    \text{Parameters:} &\hspace{10pt} \displaystyle \bm{\gamma} \in \mathbb{R}^{H}, \; \bm{\beta} \in \mathbb{R}^{H}.
\end{array}
$$
</div>


&nbsp;
## Framework


Suppose we have a vocabulary consisting of $$ V $$ distinct types (e.g., words of a language, letters of an alphabet, intensities of pixels). Let $$ \ldots, X_{-2}, X_{-1}, X_0, X_1, X_2, \ldots $$ be a seqeunce of random variables representing a sequence of tokens, where $$ X_j \in \{ 1, 2, \ldots, V \} $$ is the vocabulary ID of the $$ j $$-th token. Let $$ x_j $$ denote the observed value of $$ X_j $$. The following notation will be useful:

* $$ \bm{X}_J $$ denotes $$ (X_{j_1}, X_{j_2}, \ldots, X_{j_k}) $$ for any $$ J = (j_1, j_2, \ldots, j_k) $$; $$ \bm{x}_J $$ denotes $$ (x_{j_1}, x_{j_2}, \ldots, x_{j_k}) $$.
* $$ P(\bm{x}_J) $$ denotes $$ P(\bm{X}_J = \bm{x}_J) $$; $$ P(\bm{x}_J \mid \bm{x}_M) $$ denotes $$ P(\bm{X}_J = \bm{x}_J \mid \bm{X}_M = \bm{x}_M) $$.
* $$ i\mathord{:}j $$ denotes $$ (i, i+1, i+2, \ldots, j) $$ for any integers $$ i,j $$, with $$i\mathord{:}j $$ an empty sequence if $$ i > j $$.

A **language model** is a statistical model of the conditional distribution of $$ \bm{X}_{1:n} \mid \bm{X}_{-\infty:0} $$, i.e., the conditional distribution of a sequence of $$ n $$ tokens given a (theoretically infinite) history of tokens preceding it.

A **masked language model** is a statistical model of the conditional distribution of $$ \bm{X}_J \mid (\bm{X}_{1:n}', \bm{X}_{-\infty:0}) $$, where $$ \bm{X}_{1:n}' $$ is a corrupted sequence and $$ J $$ is a sequence of indices of the corrupted tokens. Specifically, $$ X_j' \equiv I_j \xi + (1 - I_j) X_j $$ where $$ I_j \sim \text{Bern}(p) $$ for some probability $$ p \in [0, 1] $$ of corruption and $$ \xi \in \{ 1, 2, \ldots, V \} $$ is the index of the corruption token (e.g., $$ \texttt{[MASK]} $$).


&nbsp;
## Generative Pretrained Transformer (GPT)


**GPT** is a language model under which

$$ P(\bm{x}_{1:n} \mid \bm{x}_{-\infty:0}) = \prod_{i=1}^n P(x_i \mid \bm{x}_{1:(i-1)}) \equiv \prod_{i=1}^n \text{GPT}(\bm{x}_{1:n}; \theta)[i, x_i]. $$

In this model, we assume that $$ \bm{X}_{1:n} $$ is independent of $$ \bm{X}_{-\infty:0} $$. We compute the relevant probabilities via the function $$ \text{GPT} $$ which takes as input a sequence of tokens $$ \bm{x}_{1:n} $$ and outputs an $$ n \times V $$ matrix whose $$ i $$-th row is a conditional PMF of $$ X_i $$ given the preceding tokens $$ \bm{x}_{1:(i-1)} $$. This function, parameterized by $$ \theta $$, is defined as follows:

<div class="boxed">
$$ \textbf{GPT} $$
$$
\begin{array}{@{}lll}
     \text{Input} & \bm{x} = (x_1, \ldots, x_n) \in \{1, 2, \ldots, V\}^{n} &\hspace{10pt} (n) \\
     \text{Embedding} & \bm{H}^{(0)} = \text{OneHot}(\bm{x}) \bm{W}_e + \bm{W}_p &\hspace{10pt} (n \times H) \\
     \text{Layers} & \hat{\bm{H}}{^{(l)}} = \text{LN}(\text{MH}(\bm{H}^{(l-1)}, \text{Mask}^\text{AR}) + \bm{H}^{(l-1)}), &\hspace{10pt} (n \times H) \\
     & \bm{H}^{(l)} = \text{LN}(\text{FF}^\text{GELU}(\hat{\bm{H}}{^{(l)}}) + \hat{\bm{H}}{^{(l)}}) \quad \text{for } l = 1, \ldots, L&\hspace{10pt} (n \times H) \\
     \text{Output} & \text{GPT}_\theta(\bm{x}) = \text{Softmax}\left( \bm{H}^{(L)} \bm{W}_e^\top \right) &\hspace{10pt} (n \times V)
\end{array}
$$
</div>

The set of all parameters in the GPT is given by:

$$ \theta = \{ \bm{W}_e, \bm{W}_p \} \cup \left\{ \theta_\text{MH}^{(l)}, \theta_{\text{LN},1}^{(l)}, \theta_\text{FF}^{(l)}, \theta_{\text{LN},2}^{(l)} \;\big|\; l = 1, \ldots, L \right\}, $$

where $$ \bm{W}_e \in \mathbb{R}^{V \times H} $$ and $$ \bm{W}_p \in \mathbb{R}^{n \times H} $$ are the learned embedding and positional encoding matrices, and $$ \theta_\text{MH}^{(l)} $$, $$ \theta_\text{FF}^{(l)} $$, and $$ \theta_{\text{LN},i}^{(l)} $$ are the parameters of the Multi-Head Self-Attention, Feef-Forward, and Layer Normalization modules in the $$ l $$-th layer. Correspondingly, the total number of parameters in GPT is

$$ N = VH + nH + L ( 4ADH + 2HF + 5H + F ). $$

With $$ F = 4H $$ and $$ AD = H $$:

$$ N = VH + nH + L (12 H^2 + 9H) = 12 L H^2 + (9L + V + n) H. $$

In the second iteration of the model, layer normalizations were moved to the input of each sub-block instead of the output and an additional one was added before the output softmax layer:

<div class="boxed">
$$ \textbf{GPT-2} $$
$$
\begin{array}{@{}lll}
     \text{Input} & \bm{x} = (x_1, \ldots, x_n) \in \{1, 2, \ldots, V\}^{n} &\hspace{10pt} (n) \\
     \text{Embedding} & \bm{H}^{(0)} = \text{OneHot}(\bm{x}) \bm{W}_e + \bm{W}_p &\hspace{10pt} (n \times H) \\
     \text{Layers} & \hat{\bm{H}}{^{(l)}} = \text{MH}(\text{LN}(\bm{H}^{(l-1)}), \text{Mask}^\text{AR}) + \bm{H}^{(l-1)}, &\hspace{10pt} (n \times H) \\
     & \bm{H}^{(l)} = \text{FF}^\text{GELU}(\text{LN}(\hat{\bm{H}}{^{(l)}})) + \hat{\bm{H}}{^{(l)}} \quad \text{for } l = 1, \ldots, L &\hspace{10pt} (n \times H) \\
     & \bm{F}^{(L)} = \text{LN}(\bm{H}^{(L)}) &\hspace{10pt} (n \times H) \\
     \text{Output} & \text{GPT}(\bm{x}) = \text{Softmax}\left( \bm{F}^{(L)} \bm{W}_e^\top \right) &\hspace{10pt} (n \times V)
\end{array}
$$
</div>

In the third iteration of the model, alternating dense and sparse attention patterns were used in the attention layers:

<div class="boxed">
$$ \textbf{GPT-3} $$
$$
\begin{array}{@{}lll}
     \text{Input} & \bm{x} = (x_1, \ldots, x_n) \in \{1, 2, \ldots, V\}^{n} &\hspace{10pt} (n) \\
     \text{Embedding} & \bm{H}^{(0)} = \text{OneHot}(\bm{x}) \bm{W}_e + \bm{W}_p &\hspace{10pt} (n \times H) \\
     \text{Layers} & \text{Mask}^{(l)} = \begin{cases} \text{Mask}^\text{AR} & \text{if } l \text{ is even} \\ \text{Mask}^\text{ST} & \text{if } l \text{ is odd} \end{cases} &\hspace{10pt} (n \times H) \\
     & \hat{\bm{H}}{^{(l)}} = \text{MH}(\text{LN}(\bm{H}^{(l-1)}), \text{Mask}^{(l)}) + \bm{H}^{(l-1)}, &\hspace{10pt} (n \times H) \\
     & \bm{H}^{(l)} = \text{FF}^\text{GELU}(\text{LN}(\hat{\bm{H}}{^{(l)}})) + \hat{\bm{H}}{^{(l)}} \quad \text{for } l = 1, \ldots, L &\hspace{10pt} (n \times H) \\
     & \bm{F}^{(L)} = \text{LN}(\bm{H}^{(L)}) &\hspace{10pt} (n \times H) \\
     \text{Output} & \text{GPT}(\bm{x}) = \text{Softmax}\left( \bm{F}^{(L)} \bm{W}_e^\top \right) &\hspace{10pt} (n \times V)
\end{array}
$$
</div>

GPT-2 and GPT-3 have the same parameters, given by:

$$ \theta = \{ \bm{W}_e, \bm{W}_p \} \cup \left\{ \theta_\text{MH}^{(l)}, \theta_{\text{LN},1}^{(l)}, \theta_\text{FF}^{(l)}, \theta_{\text{LN},2}^{(l)} \;\big|\; l = 1, \ldots, L \right\} \cup \left\{ \theta_{\text{LN},3}^{(L)} \right\}. $$

Hyperparameters:

$$
\begin{array}{ c c c c c c c c c }
    & N & L & H & F & D & A & V & n \\
    \hline
    \text{GPT-3 Small} & 125 \text{M} & 12 & 768 & 3072 & 64 & 12 & 50527 & 2048 \\
    \text{GPT-3 Medium} & 350 \text{M} & 24 & 1024 & 4096 & 64 & 16 & 50527 & 2048 \\
    \text{GPT-3 Large} & 760 \text{M} & 24 & 1536 & 6144 & 96 & 16 & 50527 & 2048 \\
    \text{GPT-3 XL} & 1.3 \text{B} & 24 & 2048 & 8192 & 128 & 24 & 50527 & 2048 \\
    \text{GPT-3 2.7B} & 2.7 \text{B} & 32 & 2560 & 10240 & 80 & 32 & 50527 & 2048 \\
    \text{GPT-3 6.7B} & 6.7 \text{B} & 32 & 4096 & 16384 & 128 & 32 & 50527 & 2048 \\
    \text{GPT-3 13.0B} & 13.0 \text{B} & 40 & 5120 & 20560 & 128 & 40 & 50527 & 2048 \\
    \text{GPT-3 175.0B} & 175.0 \text{B} & 96 & 12288 & 49152 & 128 & 96 & 50527 & 2048 \\
\end{array}
$$


&nbsp;
## Transformer-XL

**Transformer-XL** is a language model under which

$$ P(\bm{x}_{1:n} \mid \bm{x}_{-\infty:0}) = \prod_{i=1}^n P(x_i \mid \bm{x}_{(i-m):(i-1)}) \equiv \prod_{i=1}^n \text{TXL}(\bm{x}_{1:n}, \bm{x}_{(-m+1):0})[i, x_i], $$

where $$ m $$ is some positive integer (usually greater than $$ n $$). That is, Transformer-XL is implemented as a function that takes as input a sequence of tokens $$ \bm{x}_{1:n} $$ as well as an $$ m $$-token history $$ \bm{x}_{(-m+1):0} $$ and outputs a matrix whose $$ i $$-th row is a conditional PMF of $$ X_i $$ given $$ m $$ preceding tokens $$ \bm{x}_{(i-m):(i-1)} $$. Under this model, $$ X_j $$ is conditionally independent of $$ \bm{X}_{-\infty:(j-m-1)} $$ given $$ \bm{X}_{(j-m):(j-1)} $$.


&nbsp;
## XLNet

**XLNet** is a language model under which:

$$ P(\bm{x}_{1:n} \mid \bm{x}_{-\infty:0}) = \prod_{i=1}^n P(x_{z_i} \mid \bm{x}_{z_1:z_{i-1}}, \bm{x}_{(-m+1):0}) \equiv \prod_{i=1}^n \text{XLNet}(\bm{x}_{1:n}, \bm{x}_{(-m+1):0}, \bm{z})[z_i, x_{z_i}], $$

where $$ m $$ is some positive integer and $$ \bm{z} = (z_1, \ldots, z_n) $$ is some permutation of $$ (1, \ldots, n) $$. That is, XLNet is implemented as a function that takes as input a sequence of tokens $$ \bm{x}_{1:n} $$, an $$ m $$-token history $$ \bm{x}_{(-m+1):0} $$, and a permutation $$ \bm{z} $$, and outputs a matrix whose $$ i $$-th row is a conditional PMF of $$ X_i $$ given the tokens whose IDs come before $$ i $$ in $$ \bm{z} $$ as well as the history $$ \bm{x}_{(-m+1):0} $$. Under this model, $$ X_j $$ is conditionally independent of $$ \bm{X}_{-\infty:(j-m-1)} $$ given $$ \bm{X}_{(j-m):(j-1)} $$.

That is, we assume that $$ \bm{X}_{1:n} $$ is independent of $$ \bm{X}_{-\infty:-m} $$.

<div class="boxed">
$$ \textbf{XLnet} $$
$$
\begin{array}{@{}lll}
     \text{Input} & \bm{x} = (x_1, \ldots, x_n) \in \{1, 2, \ldots, V\}^{n} &\hspace{10pt} (n) \\
     \text{Embedding} & \bm{H}^{(0)} = \text{OneHot}(\bm{x}) \bm{W}_e &\hspace{10pt} (n \times H) \\
     & \bm{G}^{(0)} = \text{Stack}_n(\bm{w}) &\hspace{10pt} (n \times H) \\
     \text{Layers} & \bar{\bm{H}}{^{(l)}} = \text{MHA}_{\theta_\text{MH}^{(l)}}(\bm{H}^{(l-1)}, \bm{H}^{(l-1)}, \bm{H}^{(l-1)}, \text{Perm}(\text{Mask}_\text{next}^\text{AR}, \bm{z})), &\hspace{10pt} (n \times H) \\
     & \hat{\bm{H}}{^{(l)}} = \text{LN}_{\theta_\text{LN,1}^{(l)}}(\bar{\bm{H}}{^{(l)}} + \bm{H}^{(l-1)}), &\hspace{10pt} (n \times H) \\
     & \bm{H}^{(l)} = \text{LN}_{\theta_\text{LN,2}^{(l)}}(\text{FF}_{\theta_\text{FF}^{(l)}}^\text{GELU}(\hat{\bm{H}}{^{(l)}}) + \hat{\bm{H}}{^{(l)}}) \quad \text{for } l = 1, \ldots, L&\hspace{10pt} (n \times H) \\
     & \bar{\bm{G}}{^{(l)}} = \text{MHA}_{\theta_\text{MH}^{(l)}}(\bm{G}^{(l-1)}, \bm{H}^{(l-1)}, \bm{H}^{(l-1)}, \text{Perm}(\text{Mask}_\text{curr}^\text{AR}, \bm{z})), &\hspace{10pt} (n \times H) \\
     & \hat{\bm{G}}{^{(l)}} = \text{LN}_{\theta_\text{LN,1}^{(l)}}(\bar{\bm{G}}{^{(l)}} + \bm{G}^{(l-1)}), &\hspace{10pt} (n \times H) \\
     & \bm{G}^{(l)} = \text{LN}_{\theta_\text{LN,2}^{(l)}}(\text{FF}_{\theta_\text{FF}^{(l)}}^\text{GELU}(\hat{\bm{G}}{^{(l)}}) + \hat{\bm{G}}{^{(l)}}) \quad \text{for } l = 1, \ldots, L&\hspace{10pt} (n \times H) \\
     \text{Output} & \text{XLNet}_\theta(\bm{x}) = \text{Softmax}\left( \bm{G}^{(L)} \bm{W}_e^\top \right) &\hspace{10pt} (n \times V)
\end{array}
$$
</div>

<!--
<div class="boxed">
$$ \textbf{XLnet} $$
$$
\begin{array}{@{}lll}
     \text{Input} & \bm{x} = (x_1, \ldots, x_n) \in \{1, 2, \ldots, V\}^{n} &\hspace{10pt} (n) \\
     \text{Embedding} & \bm{H}^{(0)} = \text{OneHot}(\bm{x}) \bm{W}_e &\hspace{10pt} (n \times H) \\
     & \bm{G}^{(0)} = \text{Stack}_n(\bm{w}) &\hspace{10pt} (n \times H) \\
     \text{Layers} & \bar{\bm{H}}{^{(l)}}, \bar{\bm{G}}{^{(l)}} = \text{TSA}(\bm{H}^{(l-1)}, \bm{G}^{(l-1)}, \bm{z}) &\hspace{10pt} (n \times H) \text{ (both)} \\
     & \hat{\bm{H}}{^{(l)}} = \text{LN}(\text{MH}(\bm{H}^{(l-1)}, \text{Mask}^\text{AR}) + \bm{H}^{(l-1)}), &\hspace{10pt} (n \times H) \\
     & \bm{H}^{(l)} = \text{LN}(\text{FF}^\text{GELU}(\hat{\bm{H}}{^{(l)}}) + \hat{\bm{H}}{^{(l)}}) \quad \text{for } l = 1, \ldots, L&\hspace{10pt} (n \times H) \\
     \text{Output} & \text{GPT}_\theta(\bm{x}) = \text{Softmax}\left( \bm{H}^{(L)} \bm{W}_e^\top \right) &\hspace{10pt} (n \times V)
\end{array}
$$
</div> -->



&nbsp;
## BERT

* The **BERT** (**RoBERTa**) model assumes that $$ \bm{X}_{1:n} $$ is independent of $$ \bm{X}_{-\infty:0} $$ and that $$ X_{j_1}, \ldots, X_{j_k} $$ are conditionally independent given $$ \bm{X}_{1:n}' $$:

$$ P_\text{BERT}(\bm{X}_{1:n} = \bm{X}_{1:n}', \bm{X}_{-\infty:0}; \bm{\theta}) = \prod_{i=1}^k P(X_{j_i} \mid \bm{X}_{1:n}'; \bm{\theta}). $$

Given a pair of sentences $$ \bm{x}^A = (x_1^A, \ldots, x_{n_A}^A) $$ and $$ \bm{x}^B = (x_1^B, \ldots, x_{n_B}^B) $$, and an index $$ j^\text{masked} $$ at which to mask a token, let

$$ \bm{x} = (i^\text{cls}, x_1^A, \ldots, x_{n_A}^A, i^\text{sep}, x_1^B, \ldots, x_{n_B}^B, i^\text{sep} ) $$

and $$ \bm{x}' = (x'_1, \ldots, x'_n) $$ such that

$$ x'_i = \begin{cases} i^\text{mask} & \text{if } i = j^\text{masked}, \\ x_i & \text{otherwise}, \end{cases} $$

where $$ i^\text{cls} $$, $$ i^\text{sep} $$, and $$ i^\text{mask} $$ are the vocabulary indices of the classifier ($$ \texttt{[CLS]} $$), separator ($$ \texttt{[SEP]} $$), and mask ($$ \texttt{[MASK]} $$) tokens, and $$ n = n_A + n_B + 3 $$. The sequence $$ \bm{t} = (t_1, \ldots, t_n) $$ of token IDs (indicating which of the two input sentences each token belongs to) is then given by

$$ t_i = \begin{cases} 0 & \text{if } i \leq n_A + 2, \\ 1 & \text{otherwise}. \end{cases} $$

<div class="boxed">
$$ \textbf{BERT} $$
$$
\begin{array}{@{}lll}
     \text{Input} & \bm{x}' = (x'_1, \ldots, x'_n) \in \{1, 2, \ldots, V\}^{n} &\hspace{10pt} (n) \\
     & \bm{t} = (t_1, \ldots, t_n) \in \{0, 1\}^{n} &\hspace{10pt} (n) \\
     & j^\text{masked} \in \{1, 2, \ldots, n_A + n_B\} &\hspace{10pt} (1) \\
     \text{Embedding} & \bm{X}'^{(0)} = \text{OneHot}(\bm{x}') \bm{W}_e + \bm{W}_p &\hspace{10pt} (n \times H) \\
     &\bm{X}^{(0)} = \bm{X}'^{(0)} + \text{diag}(\bm{t}) \text{Stack}_n(\bm{w}^A) + \text{diag}(1 - \bm{t}) \text{Stack}_n(\bm{w}^B) &\hspace{10pt} (n \times H) \\
     \text{Layers} & \bm{X}'^{(l)} = \text{LN}(\text{MH}(\bm{X}^{(l-1)}, \text{Mask}^\text{BD}) + \bm{X}^{(l-1)}), &\hspace{10pt} (n \times H) \\
     & \bm{X}^{(l)} = \text{LN}(\text{FF}^\text{GELU}(\bm{X}'^{(l)}) + \bm{X}'^{(l)}) \quad \text{for } l = 1, \ldots, L&\hspace{10pt} (n \times H) \\
     \text{Output} & \text{BERT}(\bm{x}) = \text{Softmax}\left( \bm{X}_{j^\text{masked},:}^{(L)} \bm{W}_e^\top \right) &\hspace{10pt} (V)
\end{array}
$$
</div>

RoBERTa does not require token type IDs (due to the removal of the next sentence prediction objective) and uses $$ \texttt{<s>} $$, $$ \texttt{</s>} $$, and $$ \texttt{<mask>} $$ as classifier, separator, and mask tokens.

<div class="boxed">
$$ \textbf{RoBERTa} $$
$$
\begin{array}{@{}lll}
     \text{Input} & \bm{x}' = (x'_1, \ldots, x'_n) \in \{1, 2, \ldots, V\}^{n} &\hspace{10pt} (n) \\
     & j^\text{masked}  \in \{1, 2, \ldots, n\} &\hspace{10pt} (1) \\
     \text{Embedding} & \bm{X}^{(0)} = \text{OneHot}(\bm{x}') \bm{W}_e + \bm{W}_p &\hspace{10pt} (n \times H) \\
     \text{Layers} & \bm{X}'^{(l)} = \text{LN}(\text{MH}(\bm{X}^{(l-1)}, \text{Mask}^\text{BD}) + \bm{X}^{(l-1)}), &\hspace{10pt} (n \times H) \\
     & \bm{X}^{(l)} = \text{LN}(\text{FF}^\text{GELU}(\bm{X}'^{(l)}) + \bm{X}'^{(l)}) \quad \text{for } l = 1, \ldots, L&\hspace{10pt} (n \times H) \\
     \text{Output} & \text{RoBERTa}(\bm{x}) = \text{Softmax}\left( \bm{X}_{j^\text{masked},:}^{(L)} \bm{W}_e^\top \right) &\hspace{10pt} (V)
\end{array}
$$
</div>

The training objective is to maximize the likelihood of the masked tokens conditional on the full (bidirectional) context, which corresponds to minimizing the negative log-likelihood loss given by

$$
\begin{align*}
    \text{loss} &= - \log{P(X_{j_1} = x_{j_1}, \ldots, X_{j_m} = x_{j_m} \mid X'_1 = x'_1, \ldots, X'_n = x'_n; \bm{\theta})} \\
    &= - \sum_{i = 1}^m \log{P(X_{j_i} = x_{j_i} \mid X'_1 = x'_1, \ldots, X'_n = x'_n; \bm{\theta})} \\
    &= - \sum_{i = 1}^m \log\left( \text{Softmax}\left( \bm{X}_{j_i,:}^{(L)} \bm{W}_e^\top \right) \right),
\end{align*}
$$

under the assumption that $$ X_{j_p} $$ is independent of $$ X_{j_q} $$ given $$ (X'_1 = x'_1, \ldots, X'_n = x'_n) $$. Here $$ j_1, \ldots, j_m $$ are the indices of the masked tokens.


&nbsp;
## Hyperparameters


Vocabulary:

$$
\begin{align*}
    V &= 40\,478 & \text{byte-pair encoding.}
\end{align*}
$$

Epsilon in Layer Normalization:

$$
\begin{align*}
    \epsilon
\end{align*}
$$


Hyperparameter values (used in the paper):

$$
\begin{align*}
    & L = 12 & \text{number of layers} \\
    & H = 768 & \text{dimension of the residual stream} \\
    & F = 3072 & \text{dimension of the hidden layer in feed-forward nets} \\
    & D = 64 & \text{dimension of the vector space for queries, keys, and values} \\
    & A = 12 & \text{number of heads in multihead attention} \\
    & V = 40\,478 & \text{number of types in the vocabulary} \\
    & n_\text{ctx} = 512 & \text{context size}
\end{align*}
$$

With these values we get $$ \lambda = 116\,497\,920 $$.
